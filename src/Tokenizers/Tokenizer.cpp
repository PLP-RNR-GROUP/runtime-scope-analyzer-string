//
// Created by Даник 💪 on 30.10.2023.
//

#include "Tokenizers/Tokenizer.h"

#include <sstream>
#include <codecvt>

std::string Tokenizer::Decode(int32_t token) const {
  std::wstring coded_token = vocab_.at(token);

  std::for_each(coded_token.begin(), coded_token.end(), [this](wchar_t& character) {
    auto findResult = byte_decoder_.find(character);
    if (findResult == byte_decoder_.end()) {
      throw std::runtime_error("unknown character");
    }

    character = char(findResult->second);
  });

  return {coded_token.begin(), coded_token.end()};
}

Tokenizer::Tokenizer(const std::string& json_vocab) {
  json parsed_vocab = json::parse(json_vocab);

  // Deserialize std::map from JSON;
  for (json::iterator it = parsed_vocab.begin(); it != parsed_vocab.end(); ++it) {
    int32_t token_key = std::atoi(it.key().data());

    std::string current_token_string = it.value().template get<std::string>();

    std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;
    std::wstring wstr = converter.from_bytes(current_token_string);
    vocab_[token_key] = wstr;
  }

  byte_decoder_ = {
      {'!', 33},
      {'"', 34},
      {'#', 35},
      {'$', 36},
      {'%', 37},
      {'&', 38},
      {'\'', 39},
      {'(', 40},
      {')', 41},
      {'*', 42},
      {'+', 43},
      {',', 44},
      {'-', 45},
      {'.', 46},
      {'/', 47},
      {'0', 48},
      {'1', 49},
      {'2', 50},
      {'3', 51},
      {'4', 52},
      {'5', 53},
      {'6', 54},
      {'7', 55},
      {'8', 56},
      {'9', 57},
      {':', 58},
      {';', 59},
      {'<', 60},
      {'=', 61},
      {'>', 62},
      {'?', 63},
      {'@', 64},
      {'A', 65},
      {'B', 66},
      {'C', 67},
      {'D', 68},
      {'E', 69},
      {'F', 70},
      {'G', 71},
      {'H', 72},
      {'I', 73},
      {'J', 74},
      {'K', 75},
      {'L', 76},
      {'M', 77},
      {'N', 78},
      {'O', 79},
      {'P', 80},
      {'Q', 81},
      {'R', 82},
      {'S', 83},
      {'T', 84},
      {'U', 85},
      {'V', 86},
      {'W', 87},
      {'X', 88},
      {'Y', 89},
      {'Z', 90},
      {'[', 91},
      {'\\', 92},
      {']', 93},
      {'^', 94},
      {'_', 95},
      {'`', 96},
      {'a', 97},
      {'b', 98},
      {'c', 99},
      {'d', 100},
      {'e', 101},
      {'f', 102},
      {'g', 103},
      {'h', 104},
      {'i', 105},
      {'j', 106},
      {'k', 107},
      {'l', 108},
      {'m', 109},
      {'n', 110},
      {'o', 111},
      {'p', 112},
      {'q', 113},
      {'r', 114},
      {'s', 115},
      {'t', 116},
      {'u', 117},
      {'v', 118},
      {'w', 119},
      {'x', 120},
      {'y', 121},
      {'z', 122},
      {'{', 123},
      {'|', 124},
      {'}', 125},
      {'~', 126},
      {u'¡', 161},
      {u'¢', 162},
      {u'£', 163},
      {u'¤', 164},
      {u'¥', 165},
      {u'¦', 166},
      {u'§', 167},
      {u'¨', 168},
      {u'©', 169},
      {u'ª', 170},
      {u'«', 171},
      {u'¬', 172},
      {u'®', 174},
      {u'¯', 175},
      {u'°', 176},
      {u'±', 177},
      {u'²', 178},
      {u'³', 179},
      {u'´', 180},
      {u'µ', 181},
      {u'¶', 182},
      {u'·', 183},
      {u'¸', 184},
      {u'¹', 185},
      {u'º', 186},
      {u'»', 187},
      {u'¼', 188},
      {u'½', 189},
      {u'¾', 190},
      {u'¿', 191},
      {u'À', 192},
      {u'Á', 193},
      {u'Â', 194},
      {u'Ã', 195},
      {u'Ä', 196},
      {u'Å', 197},
      {u'Æ', 198},
      {u'Ç', 199},
      {u'È', 200},
      {u'É', 201},
      {u'Ê', 202},
      {u'Ë', 203},
      {u'Ì', 204},
      {u'Í', 205},
      {u'Î', 206},
      {u'Ï', 207},
      {u'Ð', 208},
      {u'Ñ', 209},
      {u'Ò', 210},
      {u'Ó', 211},
      {u'Ô', 212},
      {u'Õ', 213},
      {u'Ö', 214},
      {u'×', 215},
      {u'Ø', 216},
      {u'Ù', 217},
      {u'Ú', 218},
      {u'Û', 219},
      {u'Ü', 220},
      {u'Ý', 221},
      {u'Þ', 222},
      {u'ß', 223},
      {u'à', 224},
      {u'á', 225},
      {u'â', 226},
      {u'ã', 227},
      {u'ä', 228},
      {u'å', 229},
      {u'æ', 230},
      {u'ç', 231},
      {u'è', 232},
      {u'é', 233},
      {u'ê', 234},
      {u'ë', 235},
      {u'ì', 236},
      {u'í', 237},
      {u'î', 238},
      {u'ï', 239},
      {u'ð', 240},
      {u'ñ', 241},
      {u'ò', 242},
      {u'ó', 243},
      {u'ô', 244},
      {u'õ', 245},
      {u'ö', 246},
      {u'÷', 247},
      {u'ø', 248},
      {u'ù', 249},
      {u'ú', 250},
      {u'û', 251},
      {u'ü', 252},
      {u'ý', 253},
      {u'þ', 254},
      {u'ÿ', 255},
      {u'Ā', 0},
      {u'ā', 1},
      {u'Ă', 2},
      {u'ă', 3},
      {u'Ą', 4},
      {u'ą', 5},
      {u'Ć', 6},
      {u'ć', 7},
      {u'Ĉ', 8},
      {u'ĉ', 9},
      {u'Ċ', 10},
      {u'ċ', 11},
      {u'Č', 12},
      {u'č', 13},
      {u'Ď', 14},
      {u'ď', 15},
      {u'Đ', 16},
      {u'đ', 17},
      {u'Ē', 18},
      {u'ē', 19},
      {u'Ĕ', 20},
      {u'ĕ', 21},
      {u'Ė', 22},
      {u'ė', 23},
      {u'Ę', 24},
      {u'ę', 25},
      {u'Ě', 26},
      {u'ě', 27},
      {u'Ĝ', 28},
      {u'ĝ', 29},
      {u'Ğ', 30},
      {u'ğ', 31},
      {u'Ġ', 32},
      {u'ġ', 127},
      {u'Ģ', 128},
      {u'ģ', 129},
      {u'Ĥ', 130},
      {u'ĥ', 131},
      {u'Ħ', 132},
      {u'ħ', 133},
      {u'Ĩ', 134},
      {u'ĩ', 135},
      {u'Ī', 136},
      {u'ī', 137},
      {u'Ĭ', 138},
      {u'ĭ', 139},
      {u'Į', 140},
      {u'į', 141},
      {u'İ', 142},
      {u'ı', 143},
      {u'Ĳ', 144},
      {u'ĳ', 145},
      {u'Ĵ', 146},
      {u'ĵ', 147},
      {u'Ķ', 148},
      {u'ķ', 149},
      {u'ĸ', 150},
      {u'Ĺ', 151},
      {u'ĺ', 152},
      {u'Ļ', 153},
      {u'ļ', 154},
      {u'Ľ', 155},
      {u'ľ', 156},
      {u'Ŀ', 157},
      {u'ŀ', 158},
      {u'Ł', 159},
      {u'ł', 160},
      {u'Ń', 173},
  };
}


// Обвязка методов

Tokenizer* tokenizer_new(const char* json_vocab) {
  return new Tokenizer(std::string(json_vocab));
}

void tokenizer_del(Tokenizer* tokenizer) {
  delete tokenizer;
}