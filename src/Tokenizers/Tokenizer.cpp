//
// Created by Ğ”Ğ°Ğ½Ğ¸Ğº ğŸ’ª on 30.10.2023.
//

#include "Tokenizers/Tokenizer.h"

#include <sstream>
#include <codecvt>

std::string Tokenizer::Decode(int32_t token) const {
  std::wstring coded_token = vocab_.at(token);

  std::for_each(coded_token.begin(), coded_token.end(), [this](wchar_t& character) {
    auto findResult = byte_decoder_.find(character);
    if (findResult == byte_decoder_.end()) {
      throw std::runtime_error("unknown character");
    }

    character = char(findResult->second);
  });

  return {coded_token.begin(), coded_token.end()};
}

Tokenizer::Tokenizer(const std::string& json_vocab) {
  json parsed_vocab = json::parse(json_vocab);

  // Deserialize std::map from JSON;
  for (json::iterator it = parsed_vocab.begin(); it != parsed_vocab.end(); ++it) {
    int32_t token_key = std::atoi(it.key().data());

    std::string current_token_string = it.value().template get<std::string>();

    std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;
    std::wstring wstr = converter.from_bytes(current_token_string);
    vocab_[token_key] = wstr;
  }

  byte_decoder_ = {
      {'!', 33},
      {'"', 34},
      {'#', 35},
      {'$', 36},
      {'%', 37},
      {'&', 38},
      {'\'', 39},
      {'(', 40},
      {')', 41},
      {'*', 42},
      {'+', 43},
      {',', 44},
      {'-', 45},
      {'.', 46},
      {'/', 47},
      {'0', 48},
      {'1', 49},
      {'2', 50},
      {'3', 51},
      {'4', 52},
      {'5', 53},
      {'6', 54},
      {'7', 55},
      {'8', 56},
      {'9', 57},
      {':', 58},
      {';', 59},
      {'<', 60},
      {'=', 61},
      {'>', 62},
      {'?', 63},
      {'@', 64},
      {'A', 65},
      {'B', 66},
      {'C', 67},
      {'D', 68},
      {'E', 69},
      {'F', 70},
      {'G', 71},
      {'H', 72},
      {'I', 73},
      {'J', 74},
      {'K', 75},
      {'L', 76},
      {'M', 77},
      {'N', 78},
      {'O', 79},
      {'P', 80},
      {'Q', 81},
      {'R', 82},
      {'S', 83},
      {'T', 84},
      {'U', 85},
      {'V', 86},
      {'W', 87},
      {'X', 88},
      {'Y', 89},
      {'Z', 90},
      {'[', 91},
      {'\\', 92},
      {']', 93},
      {'^', 94},
      {'_', 95},
      {'`', 96},
      {'a', 97},
      {'b', 98},
      {'c', 99},
      {'d', 100},
      {'e', 101},
      {'f', 102},
      {'g', 103},
      {'h', 104},
      {'i', 105},
      {'j', 106},
      {'k', 107},
      {'l', 108},
      {'m', 109},
      {'n', 110},
      {'o', 111},
      {'p', 112},
      {'q', 113},
      {'r', 114},
      {'s', 115},
      {'t', 116},
      {'u', 117},
      {'v', 118},
      {'w', 119},
      {'x', 120},
      {'y', 121},
      {'z', 122},
      {'{', 123},
      {'|', 124},
      {'}', 125},
      {'~', 126},
      {u'Â¡', 161},
      {u'Â¢', 162},
      {u'Â£', 163},
      {u'Â¤', 164},
      {u'Â¥', 165},
      {u'Â¦', 166},
      {u'Â§', 167},
      {u'Â¨', 168},
      {u'Â©', 169},
      {u'Âª', 170},
      {u'Â«', 171},
      {u'Â¬', 172},
      {u'Â®', 174},
      {u'Â¯', 175},
      {u'Â°', 176},
      {u'Â±', 177},
      {u'Â²', 178},
      {u'Â³', 179},
      {u'Â´', 180},
      {u'Âµ', 181},
      {u'Â¶', 182},
      {u'Â·', 183},
      {u'Â¸', 184},
      {u'Â¹', 185},
      {u'Âº', 186},
      {u'Â»', 187},
      {u'Â¼', 188},
      {u'Â½', 189},
      {u'Â¾', 190},
      {u'Â¿', 191},
      {u'Ã€', 192},
      {u'Ã', 193},
      {u'Ã‚', 194},
      {u'Ãƒ', 195},
      {u'Ã„', 196},
      {u'Ã…', 197},
      {u'Ã†', 198},
      {u'Ã‡', 199},
      {u'Ãˆ', 200},
      {u'Ã‰', 201},
      {u'ÃŠ', 202},
      {u'Ã‹', 203},
      {u'ÃŒ', 204},
      {u'Ã', 205},
      {u'Ã', 206},
      {u'Ã', 207},
      {u'Ã', 208},
      {u'Ã‘', 209},
      {u'Ã’', 210},
      {u'Ã“', 211},
      {u'Ã”', 212},
      {u'Ã•', 213},
      {u'Ã–', 214},
      {u'Ã—', 215},
      {u'Ã˜', 216},
      {u'Ã™', 217},
      {u'Ãš', 218},
      {u'Ã›', 219},
      {u'Ãœ', 220},
      {u'Ã', 221},
      {u'Ã', 222},
      {u'ÃŸ', 223},
      {u'Ã ', 224},
      {u'Ã¡', 225},
      {u'Ã¢', 226},
      {u'Ã£', 227},
      {u'Ã¤', 228},
      {u'Ã¥', 229},
      {u'Ã¦', 230},
      {u'Ã§', 231},
      {u'Ã¨', 232},
      {u'Ã©', 233},
      {u'Ãª', 234},
      {u'Ã«', 235},
      {u'Ã¬', 236},
      {u'Ã­', 237},
      {u'Ã®', 238},
      {u'Ã¯', 239},
      {u'Ã°', 240},
      {u'Ã±', 241},
      {u'Ã²', 242},
      {u'Ã³', 243},
      {u'Ã´', 244},
      {u'Ãµ', 245},
      {u'Ã¶', 246},
      {u'Ã·', 247},
      {u'Ã¸', 248},
      {u'Ã¹', 249},
      {u'Ãº', 250},
      {u'Ã»', 251},
      {u'Ã¼', 252},
      {u'Ã½', 253},
      {u'Ã¾', 254},
      {u'Ã¿', 255},
      {u'Ä€', 0},
      {u'Ä', 1},
      {u'Ä‚', 2},
      {u'Äƒ', 3},
      {u'Ä„', 4},
      {u'Ä…', 5},
      {u'Ä†', 6},
      {u'Ä‡', 7},
      {u'Äˆ', 8},
      {u'Ä‰', 9},
      {u'ÄŠ', 10},
      {u'Ä‹', 11},
      {u'ÄŒ', 12},
      {u'Ä', 13},
      {u'Ä', 14},
      {u'Ä', 15},
      {u'Ä', 16},
      {u'Ä‘', 17},
      {u'Ä’', 18},
      {u'Ä“', 19},
      {u'Ä”', 20},
      {u'Ä•', 21},
      {u'Ä–', 22},
      {u'Ä—', 23},
      {u'Ä˜', 24},
      {u'Ä™', 25},
      {u'Äš', 26},
      {u'Ä›', 27},
      {u'Äœ', 28},
      {u'Ä', 29},
      {u'Ä', 30},
      {u'ÄŸ', 31},
      {u'Ä ', 32},
      {u'Ä¡', 127},
      {u'Ä¢', 128},
      {u'Ä£', 129},
      {u'Ä¤', 130},
      {u'Ä¥', 131},
      {u'Ä¦', 132},
      {u'Ä§', 133},
      {u'Ä¨', 134},
      {u'Ä©', 135},
      {u'Äª', 136},
      {u'Ä«', 137},
      {u'Ä¬', 138},
      {u'Ä­', 139},
      {u'Ä®', 140},
      {u'Ä¯', 141},
      {u'Ä°', 142},
      {u'Ä±', 143},
      {u'Ä²', 144},
      {u'Ä³', 145},
      {u'Ä´', 146},
      {u'Äµ', 147},
      {u'Ä¶', 148},
      {u'Ä·', 149},
      {u'Ä¸', 150},
      {u'Ä¹', 151},
      {u'Äº', 152},
      {u'Ä»', 153},
      {u'Ä¼', 154},
      {u'Ä½', 155},
      {u'Ä¾', 156},
      {u'Ä¿', 157},
      {u'Å€', 158},
      {u'Å', 159},
      {u'Å‚', 160},
      {u'Åƒ', 173},
  };
}


// ĞĞ±Ğ²ÑĞ·ĞºĞ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²

Tokenizer* tokenizer_new(const char* json_vocab) {
  return new Tokenizer(std::string(json_vocab));
}

void tokenizer_del(Tokenizer* tokenizer) {
  delete tokenizer;
}